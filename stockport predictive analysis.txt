



	Script for extracting tweets from Twitter

In the following part, the researcher is only including supporting libraries that will scrap tweets from twitter


import time
from selenium import webdriver
from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import csv

In the following lines, the researcher is opening Google Chrome browser through Python script and giving this browser URL of Twitter for a specific tag:

browser = webdriver.Chrome()

# write url here
# browser.get("https://twitter.com/hashtag/apple?f=news&vertical=news&src=rela")
browser.get("https://twitter.com/search?f=news&vertical=default&q=%23appl&src=typd")

In the following lines of code, Loop is running as many times as one wants to load the page of browser and get required number of tweets:
#Run this loop as many times as you want to load the page
elm = browser.find_element_by_tag_name("html") for x in range (3000):
print(x) elm.send_keys(Keys.END) time.sleep(10) elm.send_keys(Keys.HOME)
time.sleep(10)


In the following piece of code, the researcher is scrapping Tweets and Timestamp using

CSS selector and saving tweets and time in “tweet_element” and “date_element” respectively.
# scrape the content by CSS SELECTOR
tweet_element = browser.find_elements(By.CSS_SELECTOR,'p[class="TweetTextSize js- tweet-text tweet-text"]')
date_element = browser.find_elements(By.CSS_SELECTOR,'a[class="tweet-timestamp js- permalink js-nav js-tooltip"]')

Following chunk of code writes tweets along with timestamp in a csv file using function writerow():
# writing data to csv file
with open('Tweets_data_2.csv', 'w', newline='', encoding='utf-8') as csvfile: autowriter = csv.writer(csvfile)
autowriter.writerow(['Time','Tweets']) for i in range(0,len(tweet_element)-1):
tweet_text = tweet_element[i].text.encode("utf-8") autowriter.writerow([date_element[i].get_attribute("title"), tweet_text])

	Script for Cleaning/ Pre-processing of Tweets

This piece of code only imports libraries and reads dataframe from .csv file.


import numpy as np
from nltk.stem.porter import * stemmer = PorterStemmer() import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning) from nltk.corpus import stopwords
# df=pd.read_csv('data_dup.csv') df=pd.read_csv('appletweets.csv') df=df.dropna(axis=0,how='any') stop_words = stopwords.words('english')


Following code performs different cleaning operations on Twitter data which includes removal of stopwords, punctuation, special characters, tokenization, stemming,etc:
class Twitter():
def clean_tweet(self):
def remove_pattern(input, pattern): r = re.findall(pattern, input)
for i in r:
input = re.sub(i, '', input) return input


# remove twitter handles (@user)
df['cleaned_tweet'] = np.vectorize(remove_pattern)(df['Tweets'], "@[\w]*")
# remove special characters
df['cleaned_tweet'] = df['cleaned_tweet'].str.replace("[^a-zA-Z#]", " ")
# remove words less than length 3
df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda i: ' '.join([word for word in i.split() if len(word) > 3]))
# remove URLs
df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda i: re.split('https:\/\/.*', str(i))[0]) df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^A-Za-z0-9 ]+', '', regex=True)
# remove numbers
df['cleaned_tweet'] = df['cleaned_tweet'].str.replace(r'\d+', '')
# remove special character hashtag"#"
df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda i: i.replace('#', ' '))
# convert all uppercase letters to lowercase
df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda i: i.lower())
# Tokenization
tokens = df['cleaned_tweet'].apply(lambda x: x.split()) tokens.head()
tokens = tokens.apply(lambda y: [stemmer.stem(i) for i in y]) # stemming # df['tokens']=tokenized_tweet
# df.to_csv('Tokens.csv')
for i in range(len(tokens)): tokens[i] = ' '.join(tokens[i])
df['cleaned_tweet'] = tokens cleaned_tweets = tokens
# writing cleaned tweets to .csv file
df.to_csv('cleaning2.csv') return cleaned_tweets

if  name  == ' main ': tw=Twitter() tw.clean_tweet()

	Script for Labeling Tweets

Following piece of code finds out the sentiments of tweets using TextBlob library. It checks if the polarity of a tweet is less than zero then tweet is negative, if polarity is greater than zero then tweet is positive and if it is equal to zero then tweet is neutral.
from textblob import TextBlob import pandas as pd
import csv df=pd.read_csv('cleaning2.csv') cleaned_tweets=df['cleaned_tweet'] class Labelling():
def get_sentiment(self,cleaned_tweets):

# sentiment analysis of tweets using TextBlob
analysis = TextBlob(cleaned_tweets)

if analysis.sentiment.polarity > 0: return 'positive'
elif analysis.sentiment.polarity == 0: return 'neutral'
else:
return 'negative'


Following function “get_tweets()” takes cleaned tweets and store text of tweets and

sentiments in separate columns of dataframe:

def get_tweets(self):

#empty list to store tweets
list_of_tweets = []
# iterating through tweets
for tweet in cleaned_tweets:
# empty dictionary to store tweets' text and sentiment
tweet_dic = {}
# storing text part of tweet tweet_dic['Tweets'] = tweet # storing sentiment of tweet
tweet_dic['Sentiment'] = self.get_sentiment(tweet)

In main function def main (), positive tweets are labeled as “positive”, negative tweets are labeled as “negative” and neutral tweets as “neutral.” Then all tweets are written along-with their labels in csv file.
# appending parsed tweet to tweets list
if tweet_dic not in list_of_tweets: list_of_tweets.append(tweet_dic)

# return list of tweets
return list_of_tweets def main():
# creating object of Labelling Class
lab =Labelling()

# calling function to get tweets
tweets_data = lab.get_tweets()

# selecting pos tweets from all gathered tweets
ptweets_text=[tw for tw in tweets_data if tw['Sentiment'] == 'positive']

# percentage of positive tweets
print("Percentage of Positive tweets : {} %".format(100 * len(ptweets_text) / len(tweets_data)))

# picking negative tweets from tweets
ntweets_text=[tw for tw in tweets_data if tw['Sentiment'] == 'negative']

# percentage of negative tweets
print("Percentage of Negative tweets : {} %".format(100 * len(ntweets_text) / len(tweets_data)))

# percentage of neutral tweets
neutral_text=[tweet for tweet in tweets_data if tweet['Sentiment'] == 'neutral'] tweet_length = len(tweets_data)
nlength = len(ntweets_text) plength = len(ptweets_text)
print("Percentage of Neutral tweets : {} % ".format(100 * (tweet_length - nlength - plength) / tweet_length))

csv_columns=['Tweets','Sentiment']
with open('Labelled_tweets.csv', 'a') as csvfile:
writer = csv.DictWriter(csvfile, fieldnames=csv_columns) writer.writeheader()
for data in neutral_text: writer.writerow(data)

if  name  == ' main ': main()


	Model Training Using SVM

Following piece of code imports various libraries that are necessary for model training and reading data frames from .csv files.

import pandas as pd
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split from nltk.stem.porter import *
stemmer = PorterStemmer() import sys
if not sys.warnoptions: import warnings
warnings.simplefilter("ignore") df1=pd.read_csv('final_train.csv') df2=pd.read_csv('final_test.csv') df1=df1.dropna(axis=0,how='any') df2=df2.dropna(axis=0,how='any') df=pd.read_csv('output_data.csv'

               Following lines create a feature matrix for “Bag of Words” using “CountVectorizer”
            function:

bow= CountVectorizer(max_df=0.90, min_df=2, max_features=3000, stop_words='english')
# Extracting features using bag of words approach bag_of_words = bow.fit_transform(df['Tweets']) print(bag_of_words)

These lines train SVM using n-gram approach. N-grams increase the predictive power of classifiers because these find out the overall sentiment of word range i.e. two to three words.

ngram= CountVectorizer(binary=True, ngram_range=(1, 2)) ngram.fit(df['Tweets'])
X = ngram.transform(df['Tweets']) X_test = ngram.transform(df2['Tweets']) output=df['Sentiment']
X_train, X_val, y_train, y_val = train_test_split( X, output, train_size=0.75
)

c=1.0	#85.76
svm = LinearSVC(C=c) svm.fit(X_train, y_train)
print("Accuracy with SVM(ngrams) for C=%s: %s"
% (c, accuracy_score(y_val, svm.predict(X_val))))

Following piece of code train SVM using TF-IDF approach. TF-IDF increases accuracy because it gives more weightage to specific and relevant terms than irrelevant and common terms:

tfidf = TfidfVectorizer() tfidf.fit(df['Tweets'])
X = tfidf.transform(df['Tweets']) X_test = tfidf.transform(df2['Tweets']) target=df['Sentiment']
X_train, X_val, y_train, y_val = train_test_split( X, target, train_size=0.75
)

c=6.0 #86.1%
svm = LinearSVC(C=c) svm.fit(X_train, y_train)
print("Accuracy with SVM(Tfidf) for C=%s: %s"
% (c, accuracy_score(y_val, svm.predict(X_val))))

	Model Training Using SVM

Following piece of code imports various libraries that are necessary for model training and reading data frames from .csv files.

import pandas as pd
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split from nltk.stem.porter import *
stemmer = PorterStemmer() import sys
if not sys.warnoptions: import warnings
warnings.simplefilter("ignore") df1=pd.read_csv('final_train.csv') df2=pd.read_csv('final_test.csv') df1=df1.dropna(axis=0,how='any') df2=df2.dropna(axis=0,how='any') df=pd.read_csv('output_data.csv')


Following lines create a feature matrix for “Bag of Words” using “CountVectorizer”
            function:

bow= CountVectorizer(max_df=0.90, min_df=2, max_features=3000, stop_words='english')
# Extracting features using bag of words approach bag_of_words = bow.fit_transform(df['Tweets']) print(bag_of_words)

These lines train SVM using n-gram approach. N-grams increase the predictive power of classifiers because these find out the overall sentiment of word range i.e. two to three words.
ngram= CountVectorizer(binary=True, ngram_range=(1, 2)) ngram.fit(df['Tweets'])
X = ngram.transform(df['Tweets']) X_test = ngram.transform(df2['Tweets']) output=df['Sentiment']
X_train, X_val, y_train, y_val = train_test_split( X, output, train_size=0.75
)
c=1.0	#85.76
svm = LinearSVC(C=c) svm.fit(X_train, y_train)
print("Accuracy with SVM(ngrams) for C=%s: %s"
% (c, accuracy_score(y_val, svm.predict(X_val))))

Model Training using Naïve Following piece of code train SVM using TF-IDF approach. TF-IDF increases accuracy because it gives more weightage to specific and relevant terms than irrelevant and common terms:
tfidf = TfidfVectorizer() tfidf.fit(df['Tweets'])
X = tfidf.transform(df['Tweets']) X_test = tfidf.transform(df2['Tweets']) target=df['Sentiment']
X_train, X_val, y_train, y_val = train_test_split( X, target, train_size=0.75
)

c=6.0 #86.1%
svm = LinearSVC(C=c) svm.fit(X_train, y_train)
print("Accuracy with SVM(Tfidf) for C=%s: %s"
% (c, accuracy_score(y_val, svm.predict(X_val))))


	Bayes

Following lines of code just import supporting libraries for model training:

import pandas as pd import numpy as np
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB from sklearn.model_selection import train_test_split from nltk.stem.porter import *
stemmer = PorterStemmer() import re
import sys
if not sys.warnoptions: import warnings
warnings.simplefilter("ignore") df=pd.read_csv('output_data.csv')





 

